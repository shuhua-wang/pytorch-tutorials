{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01. Linear Regression\n",
    "\n",
    "#### Table of Contents\n",
    "\n",
    "- [1. Definition](#definition)\n",
    "- [2. Cost function](#costfunction)\n",
    "- [3. Gradient descent](#gradientdescent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"definition\"></a>\n",
    "\n",
    "#### 1. Definition\n",
    "\n",
    "$\\hat{y}=wx + b$\n",
    "\n",
    "$w$ - weights, $b$ - bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"costfunction\"></a>\n",
    "\n",
    "#### 2. Cost function\n",
    "\n",
    "$MSE=J(w,b)=\\frac{1}{N} \\sum_{i=1}^{n}(y_{i}-(wx_{i}+b))^{2}$\n",
    "\n",
    "Gradient of MSE with respect to $w$ and $b$ is:\n",
    "\n",
    "$J^{'}(w,b)=\\begin{bmatrix} \\frac{dJ}{dw}\\\\ \\frac{dJ}{db} \\end{bmatrix}=\\begin{bmatrix} \\frac{1}{N}\\sum -2x_{i}[y_{i}-(wx_{i}+b)]\\\\ \\frac{1}{N}\\sum -2[y_{i}-(wx_{i}+b)]\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"gradientdescent\"></a>\n",
    "\n",
    "#### 3. Gradient descent\n",
    "\n",
    "<img src=\"./images/gradient_descent.png\" alt=\"gradient_descent\" width=\"600\"/>\n",
    "\n",
    "Update rules for $w$ and $b$:\n",
    "\n",
    "$w=w - \\alpha \\frac{\\partial J(w,b)}{\\partial w}$\n",
    "\n",
    "$b=b - \\alpha \\frac{\\partial J(w,b)}{\\partial b}$\n",
    "\n",
    "$\\alpha$ is the learning rate.\n",
    "\n",
    "If $\\alpha$ is too small, gradient descent can be slow; if $\\alpha$ is too large, gradient descent can overshoot the minimum. It may fail to converge, or even diverge.\n",
    "\n",
    "<img src=\"./images/learning_rates.png\" alt=\"learning_rates\" width=\"200\"/>\n",
    "\n",
    "With low learning rates the improvements will be linear. With high learning rates they will start to look more exponential. Higher learning rates will decay the loss faster, but they get stuck at worse values of loss (green line). This is because there is too much \"Energy\" in the optimization and the parameters are bouncing around chaotically, unable to settle in a nice spot in the optimization landscape."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
