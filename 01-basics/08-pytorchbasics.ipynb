{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08. Pytorch Basics\n",
    "\n",
    "This pytorch basics tutorial contains more examples for autograd, loading data, input pipline, pretrained model, save and load model.\n",
    "\n",
    "#### Table of Contents\n",
    "\n",
    "- [1. Basic autograd example 1](#heading08-1)\n",
    "- [2. Basic autograd example 2](#heading08-2)\n",
    "- [3. Loading data from numpy](#heading08-3)\n",
    "- [4. 3 ways to stop autograd from tracking history](#heading08-4)\n",
    "- [5. Empty gradients](#heading08-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"heading08-1\"></a>\n",
    "\n",
    "#### 1. Basic autograd example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# create tensors\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# build a computational graph\n",
    "# y = 2 * x + 3\n",
    "y = w * x + b\n",
    "\n",
    "# compute gradients\n",
    "y.backward()\n",
    "\n",
    "# print out the gradients\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"heading08-2\"></a>\n",
    "\n",
    "#### 2. Basic autograd example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[-1.8100,  0.0178,  1.1026],\n",
      "        [ 0.0946, -0.1424,  0.9076],\n",
      "        [ 0.6578,  1.9524,  0.1552],\n",
      "        [-1.9348,  0.9727,  2.1199],\n",
      "        [-0.6134, -0.2425,  0.8243],\n",
      "        [-1.1508, -0.5725, -0.9671],\n",
      "        [-0.5052,  0.0695, -1.0907],\n",
      "        [ 1.3945,  1.3826, -0.6763],\n",
      "        [ 0.8802,  2.0845,  0.4626],\n",
      "        [-1.5163, -0.2028, -0.8273]])\n",
      "y: tensor([[ 0.6923, -0.2588],\n",
      "        [-0.6760, -0.3768],\n",
      "        [ 0.1214,  0.0410],\n",
      "        [ 0.4356, -0.4424],\n",
      "        [-0.1600,  0.3881],\n",
      "        [-0.6632,  1.7332],\n",
      "        [-0.4212,  0.9490],\n",
      "        [-2.0136,  1.0912],\n",
      "        [-1.4429,  2.8246],\n",
      "        [-0.4048, -0.1409]])\n"
     ]
    }
   ],
   "source": [
    "# create tensors of shape (10, 3) and (10, 2)\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "print(f'x: {x}')\n",
    "print(f'y: {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn.Linear(in_features, out_features, bias=True)` applies a linear transformation to the incoming data $y=xA^{T}+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      "tensor([[ 0.0281,  0.5275,  0.2074],\n",
      "        [ 0.2333, -0.1504, -0.4774]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([0.2383, 0.5099], requires_grad=True)\n",
      "shape of w:  torch.Size([2, 3])\n",
      "shape of b:  torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# build a fully connected layer\n",
    "linear = nn.Linear(3, 2)\n",
    "print('w: ', linear.weight)\n",
    "print('b: ', linear.bias)\n",
    "print('shape of w: ', linear.weight.size())\n",
    "print('shape of b: ', linear.bias.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4255, -0.4414],\n",
      "        [ 0.3540,  0.1201],\n",
      "        [ 1.3189,  0.2956],\n",
      "        [ 1.1366, -1.0999],\n",
      "        [ 0.2641,  0.0098],\n",
      "        [-0.2965,  0.7893],\n",
      "        [ 0.0346,  0.9024],\n",
      "        [ 0.8665,  0.9502],\n",
      "        [ 1.4585,  0.1809],\n",
      "        [-0.0828,  0.5817]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# build loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# forward pass\n",
    "pred = linear(x)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.4870531558990479\n"
     ]
    }
   ],
   "source": [
    "# compute loss\n",
    "loss = criterion(pred, y)\n",
    "print('loss: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw:  tensor([[ 0.5181,  1.2552,  0.0939],\n",
      "        [-0.0460, -0.5440, -0.2178]])\n",
      "dL/db:  tensor([ 1.0012, -0.3520])\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "loss.backward()\n",
    "\n",
    "# print out the gradients\n",
    "print('dL/dw: ', linear.weight.grad)\n",
    "print('dL/db: ', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1-step optimization:  1.4540834426879883\n"
     ]
    }
   ],
   "source": [
    "# 1-step gradient descent\n",
    "# optimizer.step()\n",
    "\n",
    "# you can also perform gradient descent at the low level\n",
    "linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# print out the loss after 1-step gradient descent\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1-step optimization: ', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"heading08-3\"></a>\n",
    "\n",
    "#### 3. Loading data from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [[1 2]\n",
      " [3 4]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# create a numpy array\n",
    "x = np.array([[1, 2], [3, 4]])\n",
    "print('x: ', x)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# convert the numpy array to a torch tensor\n",
    "y = torch.from_numpy(x)\n",
    "print('y: ', y)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z:  [[1 2]\n",
      " [3 4]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# convert the torch tensor to a numpy array\n",
    "z = y.numpy()\n",
    "print('z: ', z)\n",
    "print(type(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"heading08-4\"></a>\n",
    "\n",
    "#### 4. 3 ways to stop autograd from tracking history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x with gradient:  tensor([ 0.8552,  1.8401, -0.2896], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print('x with gradient: ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y without gradient:  tensor([2.8552, 3.8401, 1.7104])\n"
     ]
    }
   ],
   "source": [
    "# 3 ways to stop autograd from tracking history\n",
    "# 1: x.requires_grad_(False)\n",
    "# 2: x.detach()\n",
    "# 3: with torch.no_grad():\n",
    "with torch.no_grad():\n",
    "    y = x + 2\n",
    "print('y without gradient: ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"heading08-5\"></a>\n",
    "\n",
    "#### 5. Empty gradients\n",
    "\n",
    "Whenever we call the backward function then the gradient for this tensor will be accumulated into the dot grad attribute, so their values will be summed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "epoch: 0 -------------------\n",
      "model_output:  tensor(12., grad_fn=<SumBackward0>)\n",
      "weights gradient:  tensor([3., 3., 3., 3.])\n",
      "epoch: 1 -------------------\n",
      "model_output:  tensor(12., grad_fn=<SumBackward0>)\n",
      "weights gradient:  tensor([6., 6., 6., 6.])\n",
      "epoch: 2 -------------------\n",
      "model_output:  tensor(12., grad_fn=<SumBackward0>)\n",
      "weights gradient:  tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "print('weights: ', weights)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print('epoch: %d -------------------' % epoch)\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "\n",
    "    print('model_output: ', model_output)\n",
    "    print('weights gradient: ', weights.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients are summed up for each epoch. The weights' gradients are incorrect. Before the next iteration and optimization step, we must empty the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "epoch: 0 -------------------\n",
      "model_output:  tensor(12., grad_fn=<SumBackward0>)\n",
      "weights gradient:  tensor([3., 3., 3., 3.])\n",
      "epoch: 1 -------------------\n",
      "model_output:  tensor(12., grad_fn=<SumBackward0>)\n",
      "weights gradient:  tensor([3., 3., 3., 3.])\n",
      "epoch: 2 -------------------\n",
      "model_output:  tensor(12., grad_fn=<SumBackward0>)\n",
      "weights gradient:  tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "print('weights: ', weights)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print('epoch: %d -------------------' % epoch)\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "\n",
    "    print('model_output: ', model_output)\n",
    "    print('weights gradient: ', weights.grad)\n",
    "\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should also add the empty gradient in optimizer:\n",
    "\n",
    "optimizer = torch.optim.SGD(weights, lr=0.01)\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
