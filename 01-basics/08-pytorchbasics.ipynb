{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08. Pytorch Basics\n",
    "\n",
    "This pytorch basics tutorial contains more examples for autograd, loading data, input pipline, pretrained model, save and load model.\n",
    "\n",
    "#### Table of Contents\n",
    "\n",
    "- [1. Basic autograd example 1](#heading)\n",
    "- [2. Basic autograd example 2](#heading)\n",
    "- [3. Loading data from numpy](#heading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Basic autograd example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# create tensors\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# build a computational graph\n",
    "# y = 2 * x + 3\n",
    "y = w * x + b\n",
    "\n",
    "# compute gradients\n",
    "y.backward()\n",
    "\n",
    "# print out the gradients\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Basic autograd example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[-0.0748, -0.2835, -2.3969],\n",
      "        [ 1.0028, -0.3483, -1.0015],\n",
      "        [ 0.1345, -0.7380,  0.3367],\n",
      "        [ 0.3890,  0.3832,  0.7038],\n",
      "        [-0.6811, -0.6285,  0.1993],\n",
      "        [-0.6119,  0.9692, -0.6272],\n",
      "        [ 0.8341,  1.8730,  0.1967],\n",
      "        [ 0.5010, -0.9443,  0.3819],\n",
      "        [ 0.0568,  0.2196, -1.9067],\n",
      "        [-0.3059, -0.7725, -1.1613]])\n",
      "y: tensor([[ 0.6416,  1.1953],\n",
      "        [-0.6853,  1.6039],\n",
      "        [-0.7574, -0.6724],\n",
      "        [ 0.4923,  0.7073],\n",
      "        [-0.8901, -1.2171],\n",
      "        [-1.2878, -0.6040],\n",
      "        [ 0.0496, -0.1543],\n",
      "        [-0.2024, -0.8165],\n",
      "        [ 1.6689, -0.5598],\n",
      "        [ 0.2624, -1.8407]])\n"
     ]
    }
   ],
   "source": [
    "# create tensors of shape (10, 3) and (10, 2)\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "print(f'x: {x}')\n",
    "print(f'y: {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn.Linear(in_features, out_features, bias=True)` applies a linear transformation to the incoming data $y=xA^{T}+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      "tensor([[-0.4679, -0.1080,  0.5265],\n",
      "        [ 0.5600,  0.1219, -0.1688]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([-0.4259, -0.5092], requires_grad=True)\n",
      "shape of w:  torch.Size([2, 3])\n",
      "shape of b:  torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# build a fully connected layer\n",
    "linear = nn.Linear(3, 2)\n",
    "print('w: ', linear.weight)\n",
    "print('b: ', linear.bias)\n",
    "print('shape of w: ', linear.weight.size())\n",
    "print('shape of b: ', linear.bias.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6223, -0.1811],\n",
      "        [-1.3849,  0.1789],\n",
      "        [-0.2319, -0.5807],\n",
      "        [-0.2788, -0.3635],\n",
      "        [ 0.0657, -1.0010],\n",
      "        [-0.5745, -0.6279],\n",
      "        [-0.9149,  0.1530],\n",
      "        [-0.3573, -0.4082],\n",
      "        [-1.4801, -0.1288],\n",
      "        [-0.8108, -0.5787]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# build loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# forward pass\n",
    "pred = linear(x)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.354860544204712\n"
     ]
    }
   ],
   "source": [
    "# compute loss\n",
    "loss = criterion(pred, y)\n",
    "print('loss: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw:  tensor([[-0.2581, -0.1230,  1.2506],\n",
      "        [-0.1763, -0.0440,  0.1991]])\n",
      "dL/db:  tensor([-0.6882, -0.1180])\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "loss.backward()\n",
    "\n",
    "# print out the gradients\n",
    "print('dL/dw: ', linear.weight.grad)\n",
    "print('dL/db: ', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1-step optimization:  1.3329777717590332\n"
     ]
    }
   ],
   "source": [
    "# 1-step gradient descent\n",
    "# optimizer.step()\n",
    "\n",
    "# you can also perform gradient descent at the low level\n",
    "linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# print out the loss after 1-step gradient descent\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1-step optimization: ', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Loading data from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [[1 2]\n",
      " [3 4]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# create a numpy array\n",
    "x = np.array([[1, 2], [3, 4]])\n",
    "print('x: ', x)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# convert the numpy array to a torch tensor\n",
    "y = torch.from_numpy(x)\n",
    "print('y: ', y)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z:  [[1 2]\n",
      " [3 4]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# convert the torch tensor to a numpy array\n",
    "z = y.numpy()\n",
    "print('z: ', z)\n",
    "print(type(z))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
