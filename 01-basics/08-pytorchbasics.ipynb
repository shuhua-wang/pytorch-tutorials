{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08. Pytorch Basics\n",
    "\n",
    "This pytorch basics tutorial contains more examples for autograd, loading data, input pipline, pretrained model, save and load model.\n",
    "\n",
    "#### Table of Contents\n",
    "\n",
    "- [1. Basic autograd example 1](#heading)\n",
    "- [2. Basic autograd example 2](#heading)\n",
    "- [3. Loading data from numpy](#heading)\n",
    "- [4. 3 ways to stop autograd from tracking history](#heading)\n",
    "- [5. Empty gradients](#heading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Basic autograd example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# create tensors\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# build a computational graph\n",
    "# y = 2 * x + 3\n",
    "y = w * x + b\n",
    "\n",
    "# compute gradients\n",
    "y.backward()\n",
    "\n",
    "# print out the gradients\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Basic autograd example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[-0.6839, -0.2573,  0.6411],\n",
      "        [-0.4957,  0.7591, -1.2835],\n",
      "        [ 1.6182,  0.5759,  0.9798],\n",
      "        [ 1.2377, -0.3729, -1.0701],\n",
      "        [ 0.5785,  1.0547, -0.3503],\n",
      "        [-0.7050, -1.5306, -1.2690],\n",
      "        [-1.0282,  1.0397, -0.2188],\n",
      "        [ 1.8504, -0.2965, -0.8068],\n",
      "        [-0.0172,  0.4202,  0.8105],\n",
      "        [-1.2379, -0.7372, -0.9421]])\n",
      "y: tensor([[-0.6834,  0.3433],\n",
      "        [-0.0672, -1.7657],\n",
      "        [ 0.9741,  1.2022],\n",
      "        [-0.0669, -0.5877],\n",
      "        [ 1.2479, -0.4725],\n",
      "        [-0.6621, -2.0225],\n",
      "        [-1.3513, -1.3836],\n",
      "        [-1.4928, -0.7222],\n",
      "        [ 1.9421, -0.4827],\n",
      "        [ 0.3823,  0.1048]])\n"
     ]
    }
   ],
   "source": [
    "# create tensors of shape (10, 3) and (10, 2)\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "print(f'x: {x}')\n",
    "print(f'y: {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn.Linear(in_features, out_features, bias=True)` applies a linear transformation to the incoming data $y=xA^{T}+b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      "tensor([[ 0.5139,  0.1776, -0.4014],\n",
      "        [ 0.2545,  0.0679, -0.0469]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([-0.0189, -0.0841], requires_grad=True)\n",
      "shape of w:  torch.Size([2, 3])\n",
      "shape of b:  torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# build a fully connected layer\n",
    "linear = nn.Linear(3, 2)\n",
    "print('w: ', linear.weight)\n",
    "print('b: ', linear.bias)\n",
    "print('shape of w: ', linear.weight.size())\n",
    "print('shape of b: ', linear.bias.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6734, -0.3058],\n",
      "        [ 0.3763, -0.0985],\n",
      "        [ 0.5216,  0.3209],\n",
      "        [ 0.9804,  0.2558],\n",
      "        [ 0.6063,  0.1512],\n",
      "        [-0.1437, -0.3079],\n",
      "        [-0.2749, -0.2650],\n",
      "        [ 1.2032,  0.4046],\n",
      "        [-0.2785, -0.0980],\n",
      "        [-0.4079, -0.4050]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# build loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# forward pass\n",
    "pred = linear(x)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.3554081916809082\n"
     ]
    }
   ],
   "source": [
    "# compute loss\n",
    "loss = criterion(pred, y)\n",
    "print('loss: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dw:  tensor([[ 0.4499, -0.1818, -0.6026],\n",
      "        [-0.0053,  0.0010, -0.7078]])\n",
      "dL/db:  tensor([0.1687, 0.5439])\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "loss.backward()\n",
    "\n",
    "# print out the gradients\n",
    "print('dL/dw: ', linear.weight.grad)\n",
    "print('dL/db: ', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1-step optimization:  1.3412503004074097\n"
     ]
    }
   ],
   "source": [
    "# 1-step gradient descent\n",
    "# optimizer.step()\n",
    "\n",
    "# you can also perform gradient descent at the low level\n",
    "linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# print out the loss after 1-step gradient descent\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1-step optimization: ', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Loading data from numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [[1 2]\n",
      " [3 4]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# create a numpy array\n",
    "x = np.array([[1, 2], [3, 4]])\n",
    "print('x: ', x)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# convert the numpy array to a torch tensor\n",
    "y = torch.from_numpy(x)\n",
    "print('y: ', y)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z:  [[1 2]\n",
      " [3 4]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# convert the torch tensor to a numpy array\n",
    "z = y.numpy()\n",
    "print('z: ', z)\n",
    "print(type(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 3 ways to stop autograd from tracking history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x with gradient:  tensor([-0.9679,  0.3728, -1.5229], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print('x with gradient: ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y without gradient:  tensor([1.0321, 2.3728, 0.4771])\n"
     ]
    }
   ],
   "source": [
    "# 3 ways to stop autograd from tracking history\n",
    "# 1: x.requires_grad_(False)\n",
    "# 2: x.detach()\n",
    "# 3: with torch.no_grad():\n",
    "with torch.no_grad():\n",
    "    y = x + 2\n",
    "print('y without gradient: ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Empty gradients\n",
    "\n",
    "Whenever we call the backward function then the gradient for this tensor will be accumulated into the dot grad attribute, so their values will be summed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "epoch: 0 -------------------\n",
      "model_output:  tensor(12., grad_fn=<SumBackward0>)\n",
      "weights gradient:  tensor([3., 3., 3., 3.])\n",
      "epoch: 1 -------------------\n",
      "model_output:  tensor(12., grad_fn=<SumBackward0>)\n",
      "weights gradient:  tensor([6., 6., 6., 6.])\n",
      "epoch: 2 -------------------\n",
      "model_output:  tensor(12., grad_fn=<SumBackward0>)\n",
      "weights gradient:  tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "print('weights: ', weights)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print('epoch: %d -------------------' % epoch)\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "\n",
    "    print('model_output: ', model_output)\n",
    "    print('weights gradient: ', weights.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients are summed up for each epoch. The weights' gradients are incorrect. Before the next iteration and optimization step, we must empty the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "epoch: 0 -------------------\n",
      "model_output:  tensor(12., grad_fn=<SumBackward0>)\n",
      "weights gradient:  tensor([3., 3., 3., 3.])\n",
      "epoch: 1 -------------------\n",
      "model_output:  tensor(12., grad_fn=<SumBackward0>)\n",
      "weights gradient:  tensor([3., 3., 3., 3.])\n",
      "epoch: 2 -------------------\n",
      "model_output:  tensor(12., grad_fn=<SumBackward0>)\n",
      "weights gradient:  tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "print('weights: ', weights)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print('epoch: %d -------------------' % epoch)\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "\n",
    "    print('model_output: ', model_output)\n",
    "    print('weights gradient: ', weights.grad)\n",
    "\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should also add the empty gradient in optimizer:\n",
    "\n",
    "optimizer = torch.optim.SGD(weights, lr=0.01)\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
